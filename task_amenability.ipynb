{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMgz+tKrXiLpPuuTFDkQkCQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YolandaMDavis/wildtrack-iqa/blob/task-amenability/task_amenability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.17.3\n",
        "!pip install stable-baselines==2.10.2\n",
        "!pip install h5py==3.0.0\n",
        "!pip install tensorflow==1.13.1\n",
        "!pip install tensorboard==1.13.1\n",
        "!pip install Keras==2.2.4\n",
        "!pip install Keras-Applications==1.0.8\n",
        "!pip install Keras-Preprocessing==1.1.2\n",
        "!pip install keras-rl==0.4.2"
      ],
      "metadata": {
        "id": "vSVzE5j73Zsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "d4TSq2CVs964",
        "outputId": "8cfd1017-1a31-4a7f-9b6b-c5f9f396b5f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#Only needed to copy data to local drive can be skipped if zip file is already available in working folder\n",
        "import shutil\n",
        "\n",
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # for google colab. adjust accordingly\n",
        "PARENT_DIR = '/content/drive/MyDrive/Wildtrack Group/IQA' \n",
        "\n",
        "# copy and extract tar file\n",
        "shutil.copy(PARENT_DIR + '/data/WildTrack_Raw.zip', 'WildTrack_Raw.zip')\n",
        "\n",
        "# get downstream task model\n",
        "shutil.copy(PARENT_DIR + '/task-amenability/model.pt', 'model.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the repository and move data file\n",
        "!git clone https://github.com/YolandaMDavis/wildtrack-iqa.git\n",
        "!mv WildTrack_Raw.zip wildtrack-iqa/.\n",
        "!mv model.pt wildtrack-iqa/.\n",
        "%cd \"wildtrack-iqa\"\n",
        "!git checkout task-amenability\n",
        "!mv model.pt taskamenability/taskpredictor/.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyJm9Cb5t0d8",
        "outputId": "a8992131-ae72-4e4e-f3db-19d16045ef38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wildtrack-iqa'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 77 (delta 25), reused 58 (delta 13), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (77/77), done.\n",
            "/content/wildtrack-iqa\n",
            "Branch 'task-amenability' set up to track remote branch 'task-amenability' from 'origin'.\n",
            "Switched to a new branch 'task-amenability'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "# extract zip file\n",
        "with ZipFile('WildTrack_Raw.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()"
      ],
      "metadata": {
        "id": "Av0a2a-HwCvC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"wildtrack-iqa\"\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "\n",
        "with open('taskamenability/taskpredictor/class_mapping.json') as data:\n",
        "    mappings = json.load(data)\n",
        "\n",
        "class_mapping = {item['model_idx']: item['class_name'] for item in mappings}\n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('cpu')\n",
        "model = torch.jit.load('taskamenability/taskpredictor/model.pt').to(device)\n",
        "transforms = A.load('taskamenability/taskpredictor/transforms.json')"
      ],
      "metadata": {
        "id": "WmHIUgZBzneZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892329da-48ac-4839-8449-4a099effd70a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/wildtrack-iqa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils import data\n",
        "import csv\n",
        "import random\n",
        "import torch\n",
        "import yaml\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "from time import process_time\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "root_dir = '/content/wildtrack-iqa'\n",
        "data_dir = root_dir + '/RAW'\n",
        "\n",
        "def createImageSamples(img_shape, num_samples):\n",
        "  x = np.random.rand(num_samples, img_shape[0], img_shape[1], img_shape[2])\n",
        "  y = np.random.randint(low=0, high=2, size=(num_samples, 1))\n",
        "  return (x,y)\n",
        "\n",
        "def generate_data_files(sample_percentage=1):\n",
        "\n",
        "    image_reference_list = []\n",
        "    subdirectories = list(os.walk(data_dir, topdown=False))[:-1]\n",
        "\n",
        "    for subdir in subdirectories:\n",
        "        image_location = subdir[0]\n",
        "        images = subdir[2]\n",
        "        species_rating = image_location.rsplit('/', 1)[-1].replace('_', ' ')\n",
        "        score = int(species_rating.rsplit(' ', 1)[-1])\n",
        "        species_class = species_rating.rsplit(' ', 1)[:-1][0]\n",
        "        if len(species_class.rsplit(' ', 1)) > 1:\n",
        "            species = species_class.rsplit(' ')[0]\n",
        "            animal_class = ' '.join(species_class.rsplit(' ')[1:])\n",
        "        else:\n",
        "            animal_class = 'Unknown'\n",
        "            species = species_class\n",
        "\n",
        "        for image in images:\n",
        "            image_reference = (image_location, species, animal_class, image, score)\n",
        "            image_reference_list.append(image_reference)\n",
        "    \n",
        "    # add percentage of generated data\n",
        "    samples = int(len(image_reference_list) * .20)\n",
        "    for i in range(0,samples):\n",
        "      image_reference_list.append(('sample','sample','sample','sample',0))\n",
        "\n",
        "    # shuffle then split\n",
        "    seed = 1234\n",
        "    random.Random(seed).shuffle(image_reference_list)\n",
        "\n",
        "    train_index = int(len(image_reference_list) *0.6 *sample_percentage)\n",
        "    test_index = -int(len(image_reference_list) * 0.2 * sample_percentage) \n",
        "    val_index = test_index * 2\n",
        "\n",
        "    training = image_reference_list[:train_index]\n",
        "    validation = image_reference_list[val_index:test_index]\n",
        "    testing = image_reference_list[test_index:]\n",
        "    return training, validation, testing\n",
        "\n",
        "def convert_image(img_test):\n",
        "    image = Image.open(img_test)\n",
        "    image = image.convert('L')\n",
        "    image = image.resize((1024, 768))\n",
        "    image = np.array(image)\n",
        "    image = np.expand_dims(image, axis=2)\n",
        "    image = transforms(image=image)['image']\n",
        "    x = torch.from_numpy(image).to(device)\n",
        "    return x\n",
        "\n",
        "\n",
        "def transform_images(images):\n",
        "  x = []\n",
        "  y = []\n",
        "  for img in images:\n",
        "    if (img[0] == 'sample'):\n",
        "      x.append(torch.from_numpy(np.random.randint(0, 1, (416, 555, 1))))\n",
        "      y.append(0)\n",
        "    else:\n",
        "      image_name = img[0]+'/'+img[3]\n",
        "      image = convert_image(image_name)\n",
        "      x.append(image)\n",
        "      y.append(1)\n",
        "\n",
        "  return np.array(x), np.array(y)\n"
      ],
      "metadata": {
        "id": "4pAwja3NgWqL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a percentage of the full data set as a training/test/validation sample\n",
        "sample_size=.1\n",
        "train, valid, test = generate_data_files(sample_size)\n",
        "x_train, y_train = transform_images(train)\n",
        "x_val, y_val = transform_images(valid)\n",
        "x_test, y_test = transform_images(test)"
      ],
      "metadata": {
        "id": "p8GdSg7AieRs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e710f9f3-9e00-4522-afcf-f499bd86384e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:84: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:84: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execute the below installation only in Colab environment**"
      ],
      "metadata": {
        "id": "-N-KhCLnnRqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from taskamenability.interface import PPOInterface\n",
        "\n",
        "img_shape = (416,555,1)\n",
        "task_predictor = model\n",
        "interface = PPOInterface(x_train, y_train, x_val, y_val, x_test, y_test, task_predictor, img_shape)"
      ],
      "metadata": {
        "id": "dxSl5aC2xE3u"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interface.train(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "dnmi6hBOOyka",
        "outputId": "953311ca-0823-49a7-df6f-c4027ef06fd5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started for 100 episodes:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1130: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:594.)\n",
            "  return forward_call(*input, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1130: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return forward_call(*input, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1130: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at  ../torch/csrc/jit/codegen/cuda/parser.cpp:3513.)\n",
            "  return forward_call(*input, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1130: UserWarning: operator() profile_node %248 : int = prim::profile_ivalue(%out_dtype.8)\n",
            " does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:104.)\n",
            "  return forward_call(*input, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------\n",
            "| approxkl           | 11.948336  |\n",
            "| clipfrac           | 0.6422764  |\n",
            "| explained_variance | -7.68      |\n",
            "| fps                | 1          |\n",
            "| n_updates          | 1          |\n",
            "| policy_entropy     | 1.4189386  |\n",
            "| policy_loss        | 0.21729243 |\n",
            "| serial_timesteps   | 123        |\n",
            "| time_elapsed       | 1.79e-05   |\n",
            "| total_timesteps    | 123        |\n",
            "| value_loss         | 8002852.5  |\n",
            "-----------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-89230a41a620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/wildtrack-iqa/taskamenability/interface.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training started for {num_episodes} episodes:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_controller_preds_on_holdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_rollout_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;31m# true_reward is the reward without discount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mrollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;31m# Unpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines/common/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m# save final observation where user can get it, then reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/wildtrack-iqa/taskamenability/envs/task_amenability.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mmoving_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_moving_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mval_acc_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_val_acc_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mval_sel_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontroller_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mmean_val_sel_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_sel_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/wildtrack-iqa/taskamenability/envs/task_amenability.py\u001b[0m in \u001b[0;36mget_val_acc_vec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m  \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred_classes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in x_val:\n",
        "  print(item.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnUc27Yg93Nm",
        "outputId": "8f42ca64-a03b-4329-c4b0-4ae6a02a7fce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n",
            "torch.Size([416, 555, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how to extract quality mode from interface and use for prediction\n",
        "# eventually this should be a pytorch model and we'll need to account"
      ],
      "metadata": {
        "id": "flmF4Ned5oas"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}